{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests tldextract pandas beautifulsoup4 selenium urllib3 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt install -y chromium-chromedriver\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import tldextract\n",
    "\n",
    "# Aggiungi un User-Agent per simulare una richiesta da un browser reale\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:104.0) Gecko/20100101 Firefox/104.0'\n",
    "}\n",
    "\n",
    "\n",
    "def get_all_links(url, domain):\n",
    "    \"\"\"\n",
    "    Raccoglie tutti i link unici presenti in una pagina web (fornita come url),\n",
    "    filtrando solo quelli che appartengono allo stesso dominio del sito, specificato con domain.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Invia la richiesta HTTP con l'header User-Agent\n",
    "        print('**ok**')\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = set()\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            full_url = urljoin(url, href)\n",
    "            parsed_url = urlparse(full_url)\n",
    "            # Filtra i link che appartengono allo stesso dominio\n",
    "            if tldextract.extract(parsed_url.netloc).registered_domain == domain:\n",
    "                links.add(full_url)\n",
    "        return links\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return set()\n",
    "\n",
    "def crawl_website(start_url, max_depth=2):\n",
    "    \"\"\"\n",
    "    Esegue il web crawling di un sito, partendo da un'URL di base (start_url) e attraversando\n",
    "    la struttura del sito fino a un massimo di max_depth livelli di profondità.\n",
    "    \"\"\"\n",
    "    domain = tldextract.extract(start_url).registered_domain\n",
    "    visited = set()\n",
    "    to_visit = {start_url}\n",
    "    next_to_visit = set()\n",
    "\n",
    "    for depth in range(max_depth):\n",
    "        while to_visit:\n",
    "            url = to_visit.pop()\n",
    "            if url not in visited:\n",
    "                visited.add(url)\n",
    "                links = get_all_links(url, domain)\n",
    "                next_to_visit.update(links - visited)\n",
    "\n",
    "        to_visit, next_to_visit = next_to_visit, set()\n",
    "\n",
    "    return visited\n",
    "\n",
    "# Imposta l'URL iniziale e avvia il crawling\n",
    "start_url = \"https://wiki.u-gov.it/confluence/display/SCAIUS/UG3.2%3A+LEONARDO+UserGuide\"\n",
    "all_links = list(set(crawl_website(start_url, max_depth=3)))\n",
    "\n",
    "# Filtra i link per rimuovere quelli legati alla conferma dei cookie e mantenere solo le pagine in inglese\n",
    "links_simone = []\n",
    "for link in all_links:\n",
    "    if \"#editCookieSettings\" not in link:\n",
    "            links_simone.append(link)\n",
    "\n",
    "print(f\"Link trovati: {links_simone}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def extract_text_with_formatting(url):\n",
    "    try:\n",
    "        # Ottieni il contenuto della pagina\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Solleva un errore HTTP per risposte non riuscite (es. 404)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        def get_formatted_text(element):\n",
    "            \"\"\"\n",
    "            Recupera il testo con una struttura simile a Markdown\n",
    "            \"\"\"\n",
    "            tag_map = {\n",
    "                'h1': '# ', 'h2': '## ', 'h3': '### ',\n",
    "                'h4': '#### ', 'h5': '##### ', 'h6': '###### '\n",
    "            }\n",
    "\n",
    "            text = \"\"\n",
    "            if element.name in ['p', 'li']:\n",
    "                text += element.get_text() + \"\\n\"\n",
    "            elif element.name in ['ul', 'ol']:\n",
    "                for li in element.find_all('li', recursive=False):\n",
    "                    text += \"* \" + get_formatted_text(li)\n",
    "            elif element.name == 'a':\n",
    "                text += element.get_text() + f\" ({element.get('href')})\"\n",
    "            elif element.name in tag_map:\n",
    "                text += tag_map[element.name] + element.get_text() + \"\\n\"\n",
    "            elif element.name == 'table':\n",
    "                text += get_table_text(element)\n",
    "            else:\n",
    "                for child in element.children:\n",
    "                    if isinstance(child, str):\n",
    "                        text += child\n",
    "                    else:\n",
    "                        text += get_formatted_text(child)\n",
    "            return text\n",
    "\n",
    "        def get_table_text(table):\n",
    "            \"\"\"\n",
    "            Estrae il testo da una tabella HTML e lo formatta come Markdown.\n",
    "            \"\"\"\n",
    "            rows = table.find_all('tr')\n",
    "            col_widths = []\n",
    "\n",
    "            # Calcola la larghezza massima di ogni colonna\n",
    "            for row in rows:\n",
    "                cells = row.find_all(['th', 'td'])\n",
    "                for i, cell in enumerate(cells):\n",
    "                    cell_text = cell.get_text().strip()\n",
    "                    if len(col_widths) <= i:\n",
    "                        col_widths.append(len(cell_text))\n",
    "                    else:\n",
    "                        col_widths[i] = max(col_widths[i], len(cell_text))\n",
    "\n",
    "            # Crea il testo della tabella con colonne formattate\n",
    "            table_text = \"\"\n",
    "            for row in rows:\n",
    "                cells = row.find_all(['th', 'td'])\n",
    "                row_text = \"|\"\n",
    "                for i, cell in enumerate(cells):\n",
    "                    cell_text = cell.get_text().strip()\n",
    "                    row_text += f\" {cell_text.ljust(col_widths[i])} |\"\n",
    "                table_text += row_text + \"\\n\"\n",
    "\n",
    "                # Aggiungi un separatore dopo l'intestazione\n",
    "                if row == rows[0]:\n",
    "                    separator = \"|\"\n",
    "                    for width in col_widths:\n",
    "                        separator += f\" {'-' * width} |\"\n",
    "                    table_text += separator + \"\\n\"\n",
    "\n",
    "            return table_text\n",
    "\n",
    "        # Estrai il contenuto principale della pagina\n",
    "        main_content = (\n",
    "            soup.find('main') or\n",
    "            soup.find('article') or\n",
    "            soup.find('body') or\n",
    "            soup.find('div', {'id': 'content'}) or\n",
    "            soup.find('div', {'role': 'main'}) or\n",
    "            soup.find('div', {'class': 'content'}) or\n",
    "            soup.find('div', {'class': 'main-content'})\n",
    "        )\n",
    "\n",
    "        if not main_content:\n",
    "            print(f\"Main content not found for URL: {url}\")\n",
    "            return \"\"\n",
    "\n",
    "        # Estrai e pulisci il testo formattato\n",
    "        formatted_text = get_formatted_text(main_content)\n",
    "        formatted_text = formatted_text.strip()\n",
    "        formatted_text = re.sub(r'\\n{2,}', '\\n\\n', formatted_text)  # Limita le linee vuote a due\n",
    "        formatted_text = re.sub(r'\\t+', '', formatted_text)  # Rimuove le tabulazioni\n",
    "\n",
    "        return formatted_text\n",
    "\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTPError per {url}: {e}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Errore di connessione per {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "save_folder = \"output_texts\"\n",
    "os.makedirs(save_folder, exist_ok=True)  # Crea la cartella se non esiste\n",
    "\n",
    "# Processa ogni link\n",
    "for i, url in enumerate(links_simone):\n",
    "    # Estrai il testo dal link\n",
    "    formatted_text = extract_text_with_formatting(url)\n",
    "\n",
    "    # Se l'URL è valido e il testo è stato estratto correttamente\n",
    "    if formatted_text:\n",
    "        # Salva il testo in un file .txt\n",
    "        file_name = f\"file_{i}.txt\"\n",
    "        file_path = os.path.join(save_folder, file_name)\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(formatted_text)\n",
    "\n",
    "        print(f\"Testo salvato in: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Salto il link {url} a causa di un errore.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r output_texts.zip output_texts/\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"output_texts.zip\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
